{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Regularization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   displ   hp  weight  accel  size   mpg\n",
      "0  250.0   88    3139   14.5  15.0  18.0\n",
      "1  304.0  193    4732   18.5  20.0   9.0\n",
      "2   91.0   60    1800   16.4  10.0  36.1\n",
      "3  250.0   98    3525   19.0  15.0  18.5\n",
      "4   97.0   78    2188   15.8  10.0  34.3\n"
     ]
    }
   ],
   "source": [
    "# Loading the data for Mini-Challenges\n",
    "df = pd.read_csv('../data/auto.csv')\n",
    "mpg = df.iloc[:,0]\n",
    "df.drop(['origin','mpg'],axis=1,inplace=True)\n",
    "df = pd.concat([df,mpg],axis=1)\n",
    "\n",
    "# Printing the first 5 records \n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Challenge - 1\n",
    "***\n",
    "### Instructions\n",
    "* Store all independent variables in `X` variable and target varaible(i.e mpg) in `y` variable.\n",
    "* Perform a Hold-Out cross-validation by splitting the data into train and test with `random_state=9` and `test_size = 0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 5)\n",
      "(392,)\n"
     ]
    }
   ],
   "source": [
    "X = df.loc [:,['displ','hp','weight', 'accel', 'size']]\n",
    "y = df.loc [:, 'mpg']\n",
    "print (X.shape)\n",
    "print (y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the Data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6)\n",
    "\n",
    "# As it is holdout method we split training data into training and validation\n",
    "train_feat, test_feat, train_tar, test_tar = train_test_split(X_train, y_train, test_size=0.2, random_state=9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Challenge - 2\n",
    "***\n",
    "### Instructions\n",
    "* Fit a Linear Regression model on the validated dataset above and find the mean squared error for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Linear Regression:  15.514624539685553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate linear regression model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# fit model on training data\n",
    "regressor.fit(train_feat, train_tar)\n",
    "y_pred = regressor.predict(test_feat)\n",
    "\n",
    "mse=mean_squared_error(test_tar,y_pred)\n",
    "print ('Mean Squared Error for Linear Regression: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Challenge - 3\n",
    "***\n",
    "### Instructions\n",
    "* Fit a Lasso model on the same validated dataset having `alpha=140`, `max_iter=100000`, `random_state=9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lassoreg = Lasso(alpha=140,normalize=True, max_iter=1e5, random_state=9)\n",
    "lassoreg.fit(train_feat, train_tar)\n",
    "lasso_pred= lassoreg.predict(test_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Challenge - 4\n",
    "***\n",
    "### Instructions\n",
    "* Fit a Ridge model having `alpha=0.00001`, `max_iter=100000`, `random_state=9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridgereg = Ridge(alpha=0.00001,normalize=True, max_iter=1e5, random_state=9)\n",
    "ridgereg.fit(train_feat, train_tar)\n",
    "rideg_pred= ridgereg.predict(test_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Challenge - 5\n",
    "***\n",
    "### Instructions\n",
    "* Perform a k-fold cross validation with parameter `cv=5` and model as linear regression and scoring parameter as  `neg_mean_squared_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Root mean square:  56.38551015628574\n",
      "Ridge Root meam Square: 15.669132031402977\n",
      "Best Model is \n",
      "Ridge(alpha=1e-05, copy_X=True, fit_intercept=True, max_iter=100000.0,\n",
      "   normalize=True, random_state=9, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "\n",
    "# cross validation with Lasso\n",
    "rmse_lasso= abs(np.mean(cross_val_score(lassoreg, X_train, y_train, scoring=scorer, cv=5)))\n",
    "print('Lasso Root mean square: ', rmse_lasso)\n",
    "# cross validation with Ridge\n",
    "rmse_ridge= abs(np.mean(cross_val_score(ridgereg, X_train, y_train, scoring=scorer, cv=5)))\n",
    "print('Ridge Root meam Square:', rmse_ridge)\n",
    "# select best model\n",
    "Model = lassoreg if rmse_lasso<rmse_ridge else ridgereg\n",
    "print (\"Best Model is \", Model, sep =\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Linear Regression with Regularisation is :  24.76387714705963\n"
     ]
    }
   ],
   "source": [
    "model_pred = Model.predict(X_test)\n",
    "mse=mean_squared_error(y_test,model_pred)\n",
    "print ('Mean Squared Error for Linear Regression with Regularisation is : ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Q1. Which of the following statement(s) is / are true for Gradient Decent (GD) and Stochastic Gradient Decent (SGD)?\n",
    "```python\n",
    "1. In GD and SGD, you update a set of parameters in an iterative manner to minimize the error function. \n",
    "2. In SGD, you have to run through all the samples in your training set for a single update of a parameter \n",
    "   in each    iteration. \n",
    "3. In GD, you either use the entire data or a subset of training data to update a parameter in each \n",
    "   iteration. \n",
    "\n",
    "A) Only 1\n",
    "B) Only 2\n",
    "C) Only 3\n",
    "D) 1 and 2\n",
    "E) 2 and 3\n",
    "F) 1,2 and 3\n",
    "\n",
    "Ans: A\n",
    "\n",
    "```\n",
    "Q2. Which of the following options is/are true for K-fold cross-validation?\n",
    "```python\n",
    "1. Increase in K will result in higher time required to cross validate the result.\n",
    "2. Higher values of K will result in higher confidence on the cross-validation result as compared to lower \n",
    "   value of K.\n",
    "3. If K=1, then it is called Leave one out cross validation, where N is the number of observations.\n",
    " \n",
    "\n",
    "A) 1 and 2\n",
    "B) 2 and 3\n",
    "C) 1 and 3\n",
    "D) 1,2 and 3\n",
    "\n",
    "Ans: D\n",
    "\n",
    "```\n",
    "Q3. Which of the following is true about “Ridge” or “Lasso” regression methods in case of feature selection?\n",
    "```python\n",
    "A. Ridge regression uses subset selection of features\n",
    "B. Lasso regression uses subset selection of features\n",
    "C. Both use subset selection of features\n",
    "D. None of above\n",
    "\n",
    "Ans: B\n",
    "    \n",
    "    \n",
    "```\n",
    "Q4. What is/are true about ridge regression?\n",
    "```python\n",
    "1. When lambda is 0, model works like linear regression model\n",
    "2. When lambda is 0, model doesn’t work like linear regression model\n",
    "3. When lambda goes to infinity, we get very, very small coefficients approaching 0\n",
    "4. When lambda goes to infinity, we get very, very large coefficients approaching infinity\n",
    "\n",
    "A. 1 and 3\n",
    "B. 1 and 4\n",
    "C. 2 and 3\n",
    "D. 2 and 4\n",
    "\n",
    "Ans: A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
